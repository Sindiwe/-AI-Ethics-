# -*- coding: utf-8 -*-
"""Part3_Practical Audit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UXs_zgTIH6TKXt0OkRQN1JBtUcUvmOUO
"""

# Install aif360 if not already installed
# !pip install aif360

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from aif360.datasets import CompasDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

# Load COMPAS dataset from AIF360
dataset = CompasDataset()

# Split dataset into train and test
train, test = dataset.split([0.7], shuffle=True)

# Check basic stats
print("Training dataset shape before cleaning:", train.features.shape)
print("Test dataset shape before cleaning:", test.features.shape)

# Define privileged and unprivileged groups
privileged_groups = [{'race': 1}]   # 1 = White
unprivileged_groups = [{'race': 0}] # 0 = African-American

# Data Cleaning: Remove rows with inf or NaN values in features or labels
train_features = pd.DataFrame(train.features)
train_labels = pd.DataFrame(train.labels)
train_protected = pd.DataFrame(train.protected_attributes)

train_combined = pd.concat([train_features, train_labels, train_protected], axis=1)
train_combined.replace([np.inf, -np.inf], np.nan, inplace=True)
train_combined.dropna(inplace=True)

train.features = train_combined.iloc[:, :train_features.shape[1]].values
train.labels = train_combined.iloc[:, train_features.shape[1]:train_features.shape[1]+train_labels.shape[1]].values
train.protected_attributes = train_combined.iloc[:, train_features.shape[1]+train_labels.shape[1]:].values


test_features = pd.DataFrame(test.features)
test_labels = pd.DataFrame(test.labels)
test_protected = pd.DataFrame(test.protected_attributes)

test_combined = pd.concat([test_features, test_labels, test_protected], axis=1)
test_combined.replace([np.inf, -np.inf], np.nan, inplace=True)
test_combined.dropna(inplace=True)

test.features = test_combined.iloc[:, :test_features.shape[1]].values
test.labels = test_combined.iloc[:, test_features.shape[1]:test_features.shape[1]+test_labels.shape[1]].values
test.protected_attributes = test_combined.iloc[:, test_features.shape[1]+test_labels.shape[1]:].values


print("Training dataset shape after cleaning:", train.features.shape)
print("Test dataset shape after cleaning:", test.features.shape)


# Bias metric before mitigation
metric_train = BinaryLabelDatasetMetric(train,
                                       unprivileged_groups=unprivileged_groups,
                                       privileged_groups=privileged_groups)

print("Difference in mean outcomes (train): %.4f" % metric_train.mean_difference())

# Train logistic regression without mitigation
X_train = train.features
y_train = train.labels.ravel()
X_test = test.features
y_test = test.labels.ravel()

model = LogisticRegression(solver='liblinear')
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Wrap test dataset with predictions for fairness metrics
test_pred = test.copy()
test_pred.labels = y_pred

# Classification metric for fairness evaluation
metric_test = ClassificationMetric(test, test_pred,
                                   unprivileged_groups=unprivileged_groups,
                                   privileged_groups=privileged_groups)

# Calculate false positive rates for each group using confusion matrix
# Filter data for unprivileged group
test_unprivileged_indices = test.protected_attributes[:, test.protected_attribute_names.index('race')] == unprivileged_groups[0]['race']
y_test_unprivileged = y_test[test_unprivileged_indices]
y_pred_unprivileged = y_pred[test_unprivileged_indices]

# Filter data for privileged group
test_privileged_indices = test.protected_attributes[:, test.protected_attribute_names.index('race')] == privileged_groups[0]['race']
y_test_privileged = y_test[test_privileged_indices]
y_pred_privileged = y_pred[test_privileged_indices]

# Calculate confusion matrices
tn_u, fp_u, fn_u, tp_u = confusion_matrix(y_test_unprivileged, y_pred_unprivileged).ravel()
tn_p, fp_p, fn_p, tp_p = confusion_matrix(y_test_privileged, y_pred_privileged).ravel()

# Calculate false positive rates
fpr_unprivileged = fp_u / (fp_u + tn_u) if (fp_u + tn_u) > 0 else 0
fpr_privileged = fp_p / (fp_p + tn_p) if (fp_p + tn_p) > 0 else 0
fpr_difference = fpr_unprivileged - fpr_privileged


print("False positive rate (unprivileged): %.4f" % fpr_unprivileged)
print("False positive rate (privileged): %.4f" % fpr_privileged)
print("False positive rate difference: %.4f" % fpr_difference)


# Visualization of False Positive Rates

groups = ['Unprivileged (African-American)', 'Privileged (White)']
fpr = [fpr_unprivileged, fpr_privileged]

plt.bar(groups, fpr, color=['red', 'green'])
plt.ylabel('False Positive Rate')
plt.title('False Positive Rate by Race - COMPAS Dataset')
plt.show()

# Bias Mitigation: Reweighing
RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)
RW.fit(train)
train_transformed = RW.transform(train)

# Train model on reweighed data
model_rw = LogisticRegression(solver='liblinear')
model_rw.fit(train_transformed.features, train_transformed.labels.ravel(), sample_weight=train_transformed.instance_weights)

# Predict on test set with original features (no weights)
y_pred_rw = model_rw.predict(X_test)
test_pred_rw = test.copy()
test_pred_rw.labels = y_pred_rw

# Evaluate after mitigation
metric_test_rw = ClassificationMetric(test, test_pred_rw,
                                      unprivileged_groups=unprivileged_groups,
                                      privileged_groups=privileged_groups)

# Calculate false positive rates after reweighing using confusion matrix
# Filter data for unprivileged group
y_test_unprivileged_rw = y_test[test_unprivileged_indices]
y_pred_unprivileged_rw = y_pred_rw[test_unprivileged_indices]

# Filter data for privileged group
y_test_privileged_rw = y_test[test_privileged_indices]
y_pred_privileged_rw = y_pred_rw[test_privileged_indices]

# Calculate confusion matrices
tn_u_rw, fp_u_rw, fn_u_rw, tp_u_rw = confusion_matrix(y_test_unprivileged_rw, y_pred_unprivileged_rw).ravel()
tn_p_rw, fp_p_rw, fn_p_rw, tp_p_rw = confusion_matrix(y_test_privileged_rw, y_pred_privileged_rw).ravel()

# Calculate false positive rates
fpr_unprivileged_rw = fp_u_rw / (fp_u_rw + tn_u_rw) if (fp_u_rw + tn_u_rw) > 0 else 0
fpr_privileged_rw = fp_p_rw / (fp_p_rw + tn_p_rw) if (fp_p_rw + tn_p_rw) > 0 else 0
fpr_difference_rw = fpr_unprivileged_rw - fpr_privileged_rw


print("\nAfter Reweighing Mitigation:")
print("False positive rate (unprivileged): %.4f" % fpr_unprivileged_rw)
print("False positive rate (privileged): %.4f" % fpr_privileged_rw)
print("False positive rate difference: %.4f" % fpr_difference_rw)

# Visualize again after mitigation
fpr_rw = [fpr_unprivileged_rw, fpr_privileged_rw]

plt.bar(groups, fpr_rw, color=['red', 'green'])
plt.ylabel('False Positive Rate')
plt.title('False Positive Rate by Race after Reweighing')
plt.show()

!pip install aif360

# Download the COMPAS dataset
!wget https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv

# Define the target directory for the dataset
import os
target_dir = '/usr/local/lib/python3.11/dist-packages/aif360/data/raw/compas'

# Create the directory if it doesn't exist
os.makedirs(target_dir, exist_ok=True)

# Move the downloaded file to the target directory
!mv compas-scores-two-years.csv {target_dir}